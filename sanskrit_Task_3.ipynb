{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T10:28:28.693090Z","iopub.execute_input":"2025-05-24T10:28:28.693336Z","iopub.status.idle":"2025-05-24T10:28:31.994686Z","shell.execute_reply.started":"2025-05-24T10:28:28.693311Z","shell.execute_reply":"2025-05-24T10:28:31.989219Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def sanskrit_text_identification_rl():\n#     # Initialize LLM with base knowledge of Sanskrit literature\n#     model = SanskritCapableLLM()\n    \n#     # Parameters\n#     learning_rate = 0.01\n#     episodes = 10000\n    \n#     for episode in range(episodes):\n#         # Sample a random text from GRETIL corpus\n#         quote, true_source = sample_from_gretil()\n        \n#         # Get model's prediction\n#         prediction = model.identify_source(quote)\n        \n#         # Calculate reward based on accuracy of identification\n#         reward = calculate_reward(prediction, true_source)\n        \n#         # Update model weights using reward signal\n#         model.update(reward, learning_rate)\n        \n#         # Periodically evaluate and log performance\n#         if episode % 100 == 0:\n#             evaluate_performance(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:59:23.217000Z","iopub.execute_input":"2025-05-24T13:59:23.217300Z","iopub.status.idle":"2025-05-24T13:59:23.228599Z","shell.execute_reply.started":"2025-05-24T13:59:23.217276Z","shell.execute_reply":"2025-05-24T13:59:23.223370Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def calculate_reward(prediction, true_source):\n#     reward = 0\n    \n#     # Exact identification (text, chapter, verse)\n#     if prediction.exact_match(true_source):\n#         reward = 10.0\n    \n#     # Correct text and chapter but wrong verse\n#     elif prediction.text == true_source.text and prediction.chapter == true_source.chapter:\n#         reward = 7.0\n    \n#     # Correct text but wrong chapter\n#     elif prediction.text == true_source.text:\n#         reward = 5.0\n    \n#     # Correct genre/category\n#     elif prediction.genre == true_source.genre:\n#         reward = 2.0\n    \n#     # Completely wrong\n#     else:\n#         reward = -1.0\n    \n#     # Additional reward for stylistic recognition\n#     if prediction.has_style_recognition():\n#         reward += 1.0\n        \n#     return reward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:59:36.007814Z","iopub.execute_input":"2025-05-24T13:59:36.008119Z","iopub.status.idle":"2025-05-24T13:59:36.018056Z","shell.execute_reply.started":"2025-05-24T13:59:36.008076Z","shell.execute_reply":"2025-05-24T13:59:36.014759Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# def sample_from_gretil():\n#     # Initially uniform sampling across all texts\n#     all_texts = load_gretil_corpus()\n    \n#     # As training progresses, implement curriculum learning:\n#     # Start with distinctive verses, gradually introduce more difficult ones\n#     if training_progress < 0.3:  # First 30% of training\n#         sampled_text = sample_distinctive_verses(all_texts)\n#     elif training_progress < 0.7:  # Middle 40% of training\n#         sampled_text = sample_moderate_verses(all_texts)\n#     else:  # Final 30% of training\n#         sampled_text = sample_difficult_verses(all_texts)\n    \n#     quote = extract_quote(sampled_text)\n#     source_info = get_metadata(sampled_text)\n    \n#     return quote, source_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T13:59:44.912960Z","iopub.execute_input":"2025-05-24T13:59:44.913273Z","iopub.status.idle":"2025-05-24T13:59:44.924810Z","shell.execute_reply.started":"2025-05-24T13:59:44.913246Z","shell.execute_reply":"2025-05-24T13:59:44.919436Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# def evaluate_performance(model):\n#     test_quotes = get_evaluation_set()\n    \n#     accuracy_metrics = {\n#         \"exact_match\": 0,\n#         \"text_match\": 0,\n#         \"genre_match\": 0\n#     }\n    \n#     for quote, true_source in test_quotes:\n#         prediction = model.identify_source(quote)\n        \n#         if prediction.exact_match(true_source):\n#             accuracy_metrics[\"exact_match\"] += 1\n        \n#         if prediction.text == true_source.text:\n#             accuracy_metrics[\"text_match\"] += 1\n            \n#         if prediction.genre == true_source.genre:\n#             accuracy_metrics[\"genre_match\"] += 1\n    \n#     # Calculate percentages\n#     for key in accuracy_metrics:\n#         accuracy_metrics[key] = accuracy_metrics[key] / len(test_quotes) * 100\n        \n#     log_metrics(accuracy_metrics)\n#     return accuracy_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T14:00:02.308729Z","iopub.execute_input":"2025-05-24T14:00:02.309001Z","iopub.status.idle":"2025-05-24T14:00:02.320997Z","shell.execute_reply.started":"2025-05-24T14:00:02.308977Z","shell.execute_reply":"2025-05-24T14:00:02.315775Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# import random\n\n# # Initialize model and memory buffer\n# class SanskritCapableLLM:\n#     def identify_source(self, quote):\n#         # Placeholder for model prediction\n#         return PredictionResult(\"unknown\", \"unknown\", 0)\n    \n#     def update(self, reward, learning_rate):\n#         # Placeholder for model update\n#         pass\n    \n#     def update_from_memory(self, quote, prediction, reward, true_source):\n#         # Placeholder for learning from memory\n#         pass\n\n# class PredictionResult:\n#     def __init__(self, text, chapter, verse, genre=\"unknown\"):\n#         self.text = text\n#         self.chapter = chapter\n#         self.verse = verse\n#         self.genre = genre\n    \n#     def exact_match(self, other):\n#         return (self.text == other.text and \n#                 self.chapter == other.chapter and \n#                 self.verse == other.verse)\n\n# class SourceInfo:\n#     def __init__(self, text, chapter, verse, genre):\n#         self.text = text\n#         self.chapter = chapter\n#         self.verse = verse\n#         self.genre = genre\n\n# def sample_from_gretil():\n#     # Mock function to sample text from GRETIL\n#     texts = [\"Bhagavad Gita\", \"Ramayana\", \"Mahabharata\"]\n#     genres = [\"Epic\", \"Philosophy\", \"Religious\"]\n    \n#     text = random.choice(texts)\n#     chapter = random.randint(1, 18)\n#     verse = random.randint(1, 100)\n#     genre = random.choice(genres)\n    \n#     quote = f\"Sample quote from {text} {chapter}.{verse}\"\n#     source = SourceInfo(text, chapter, verse, genre)\n    \n#     return quote, source\n\n# def calculate_reward(prediction, true_source):\n#     # Simple reward function\n#     if prediction.exact_match(true_source):\n#         return 10.0\n#     elif prediction.text == true_source.text:\n#         return 5.0\n#     elif prediction.genre == true_source.genre:\n#         return 2.0\n#     else:\n#         return -1.0\n\n# def sample_batch(buffer, batch_size=10):\n#     return random.sample(buffer, min(batch_size, len(buffer)))\n\n# # Main training loop\n# def train_sanskrit_librarian():\n#     model = SanskritCapableLLM()\n#     memory_buffer = []\n#     learning_rate = 0.01\n#     episodes = 100\n    \n#     for episode in range(episodes):\n#         # Get sample\n#         quote, true_source = sample_from_gretil()\n        \n#         # Get prediction\n#         prediction = model.identify_source(quote)\n        \n#         # Calculate reward\n#         reward = calculate_reward(prediction, true_source)\n        \n#         # Update model\n#         model.update(reward, learning_rate)\n        \n#         # Store in memory buffer\n#         memory_buffer.append((quote, prediction, reward, true_source))\n        \n#         # Experience replay\n#         if episode % 10 == 0 and len(memory_buffer) > 10:\n#             batch = sample_batch(memory_buffer)\n#             for q, p, r, t in batch:\n#                 model.update_from_memory(q, p, r, t)\n        \n#         # Print progress\n#         if episode % 10 == 0:\n#             print(f\"Episode {episode}: Processing quote from {true_source.text}\")\n\n# if __name__ == \"__main__\":\n#     train_sanskrit_librarian()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T14:01:04.870710Z","iopub.execute_input":"2025-05-24T14:01:04.871083Z","iopub.status.idle":"2025-05-24T14:01:04.891371Z","shell.execute_reply.started":"2025-05-24T14:01:04.871054Z","shell.execute_reply":"2025-05-24T14:01:04.886247Z"}},"outputs":[{"name":"stdout","text":"Episode 0: Processing quote from Bhagavad Gita\nEpisode 10: Processing quote from Bhagavad Gita\nEpisode 20: Processing quote from Bhagavad Gita\nEpisode 30: Processing quote from Ramayana\nEpisode 40: Processing quote from Mahabharata\nEpisode 50: Processing quote from Bhagavad Gita\nEpisode 60: Processing quote from Mahabharata\nEpisode 70: Processing quote from Ramayana\nEpisode 80: Processing quote from Mahabharata\nEpisode 90: Processing quote from Mahabharata\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# # Encourage model to explore less familiar texts\n# def calculate_exploration_bonus(quote):\n#     familiarity = model.estimate_familiarity(quote)\n#     return 1.0 - familiarity  # Higher bonus for less familiar texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T14:01:33.082629Z","iopub.execute_input":"2025-05-24T14:01:33.082928Z","iopub.status.idle":"2025-05-24T14:01:33.092170Z","shell.execute_reply.started":"2025-05-24T14:01:33.082900Z","shell.execute_reply":"2025-05-24T14:01:33.088077Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:02:44.464861Z","iopub.execute_input":"2025-05-24T15:02:44.465193Z"}},"outputs":[{"name":"stdout","text":"Collecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk) (4.67.1)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk) (2024.11.6)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk) (1.5.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk) (8.1.8)\nInstalling collected packages: nltk\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:05:58.711944Z","iopub.execute_input":"2025-05-24T15:05:58.712380Z","iopub.status.idle":"2025-05-24T15:06:02.644765Z","shell.execute_reply.started":"2025-05-24T15:05:58.712347Z","shell.execute_reply":"2025-05-24T15:06:02.640169Z"}},"outputs":[{"name":"stdout","text":"Collecting tiktoken\n  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\nInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.9.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!pip install transformers_stream_generator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:06:31.911431Z","iopub.execute_input":"2025-05-24T15:06:31.911826Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers_stream_generator\n  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.10/site-packages (from transformers_stream_generator) (4.51.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (2.0.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.21.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (3.18.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.5.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (2.32.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (25.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.31.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.26.1->transformers_stream_generator) (1.1.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.26.1->transformers_stream_generator) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.26.1->transformers_stream_generator) (4.13.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2025.4.26)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.4.0)\nBuilding wheels for collected packages: transformers_stream_generator\n  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12524 sha256=66e6257b4bf72215a8c1a83601bb4c884ddce29589eb82d4c3437a0619b6606a\n  Stored in directory: /root/.cache/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\nSuccessfully built transformers_stream_generator\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Advanced Sanskrit Text Identification System with Qwen Model\n\n# ```python\nimport torch\nimport numpy as np\nimport pandas as pd\nimport random\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\nfrom scipy.spatial.distance import cosine\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom tqdm import tqdm\nfrom collections import Counter\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\nimport re\nimport json\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass QwenSanskritLibrarian:\n    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        logger.info(f\"Initializing QwenSanskritLibrarian with {model_name} on {device}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name, \n            device_map=device,\n            trust_remote_code=True\n        )\n        self.device = device\n        \n        # Text embeddings cache\n        self.text_embeddings = {}\n        \n        # Confidence tracking\n        self.confidence_history = []\n        \n        # Sanskrit linguistic patterns\n        self.load_sanskrit_patterns()\n        \n        # Performance metrics\n        self.metrics = {\n            \"response_times\": [],\n            \"reward_history\": [],\n            \"genre_f1_scores\": {},\n            \"text_accuracies\": {},\n            \"epoch_accuracies\": []\n        }\n        \n    def load_sanskrit_patterns(self):\n        \"\"\"Load patterns that are characteristic of different Sanskrit texts and genres\"\"\"\n        # This would be a comprehensive database of patterns, keywords, phrases\n        self.patterns = {\n            \"Bhagavad Gita\": {\n                \"keywords\": [\"karma\", \"dharma\", \"yoga\", \"arjuna\", \"krishna\"],\n                \"verse_pattern\": r\"अहं|योगः|धर्मः\",\n                \"style_markers\": [\"philosophical dialogue\", \"moral instruction\"]\n            },\n            \"Ramayana\": {\n                \"keywords\": [\"rama\", \"sita\", \"hanuman\", \"ravana\", \"ayodhya\"],\n                \"verse_pattern\": r\"रामः|सीता|लंका\",\n                \"style_markers\": [\"narrative epic\", \"moral exemplar\"]\n            },\n            # Would include many more texts\n        }\n        \n    def preprocess_sanskrit(self, text):\n        \"\"\"Preprocess Sanskrit text for better analysis\"\"\"\n        # Convert various Unicode representations to standard\n        text = self.normalize_unicode(text)\n        # Handle sandhi (word combinations)\n        text = self.separate_sandhi(text)\n        return text\n        \n    def normalize_unicode(self, text):\n        \"\"\"Normalize different Unicode representations of Sanskrit\"\"\"\n        # Placeholder for complex Unicode normalization\n        return text\n        \n    def separate_sandhi(self, text):\n        \"\"\"Handle Sanskrit sandhi (word combinations)\"\"\"\n        # Placeholder for sandhi splitting algorithm\n        return text\n    \n    def identify_source(self, quote, top_k=5):\n        \"\"\"Identify the source of a Sanskrit quote\"\"\"\n        start_time = time.time()\n        \n        # Preprocess the quote\n        processed_quote = self.preprocess_sanskrit(quote)\n        \n        # Construct prompt\n        prompt = f\"\"\"\n        As a Sanskrit scholar, identify the source of this quote with as much detail as possible.\n        Provide the text name, chapter/section, and verse number if applicable.\n        \n        Quote: \"{processed_quote}\"\n        \n        The source is:\n        \"\"\"\n        \n        # Generate response from model\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs.input_ids,\n                max_new_tokens=512,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True,\n                num_return_sequences=top_k\n            )\n        \n        # Process multiple candidate answers\n        candidates = []\n        for output in outputs:\n            response_text = self.tokenizer.decode(output, skip_special_tokens=True)\n            response = response_text.split(\"The source is:\")[1].strip()\n            \n            # Extract structured information\n            extracted = self.extract_source_info(response)\n            \n            # Calculate confidence score based on response\n            confidence = self.calculate_confidence(response, processed_quote)\n            \n            candidates.append({\n                \"prediction\": extracted,\n                \"raw_response\": response,\n                \"confidence\": confidence\n            })\n        \n        # Sort by confidence\n        candidates.sort(key=lambda x: x[\"confidence\"], reverse=True)\n        \n        # Record response time\n        response_time = time.time() - start_time\n        self.metrics[\"response_times\"].append(response_time)\n        \n        # Record confidence of top prediction\n        self.confidence_history.append(candidates[0][\"confidence\"])\n        \n        return candidates[0][\"prediction\"], candidates\n    \n    def extract_source_info(self, response):\n        \"\"\"Extract structured source information from model response\"\"\"\n        # Use regex patterns to extract text, chapter, verse\n        text_pattern = r\"(from|in|is)\\s+(?:the)?\\s*([A-Za-z\\s]+?)(?:,|\\.|in Chapter|\\(|\\n|$)\"\n        chapter_pattern = r\"Chapter\\s+(\\d+)\"\n        verse_pattern = r\"[Vv]erse\\s+(\\d+)\"\n        \n        text_match = re.search(text_pattern, response)\n        chapter_match = re.search(chapter_pattern, response)\n        verse_match = re.search(verse_pattern, response)\n        \n        # Extract genre through keyword analysis\n        genre = self.determine_genre(response)\n        \n        result = PredictionResult(\n            text=text_match.group(2).strip() if text_match else \"unknown\",\n            chapter=int(chapter_match.group(1)) if chapter_match else 0,\n            verse=int(verse_match.group(1)) if verse_match else 0,\n            genre=genre,\n            confidence=0.0  # Will be set later\n        )\n        \n        return result\n    \n    def determine_genre(self, response):\n        \"\"\"Determine the genre based on response content\"\"\"\n        genre_keywords = {\n            \"Epic\": [\"epic\", \"mahabharata\", \"ramayana\", \"heroic\"],\n            \"Philosophy\": [\"philosophy\", \"philosophical\", \"darshana\", \"vedanta\"],\n            \"Religious\": [\"religious\", \"devotional\", \"bhakti\", \"spiritual\"],\n            \"Technical\": [\"technical\", \"grammar\", \"linguistics\", \"science\", \"shastra\"],\n            \"Poetry\": [\"poetry\", \"poetic\", \"kavya\", \"aesthetic\"]\n        }\n        \n        # Count occurrences of each genre's keywords\n        counts = {genre: 0 for genre in genre_keywords}\n        for genre, keywords in genre_keywords.items():\n            for keyword in keywords:\n                counts[genre] += len(re.findall(r'\\b' + keyword + r'\\b', response.lower()))\n        \n        # Return the genre with the most keyword matches\n        if max(counts.values()) > 0:\n            return max(counts, key=counts.get)\n        else:\n            return \"Unknown\"\n    \n    def calculate_confidence(self, response, quote):\n        \"\"\"Calculate confidence score for the prediction\"\"\"\n        # Factors that increase confidence:\n        confidence = 0.5  # Base confidence\n        \n        # 1. Specificity of the answer\n        if re.search(r\"Chapter\\s+\\d+\", response):\n            confidence += 0.1\n        if re.search(r\"[Vv]erse\\s+\\d+\", response):\n            confidence += 0.1\n        \n        # 2. Consistency in the answer\n        text_mentions = re.findall(r\"([A-Za-z\\s]+?)(?:,|\\.|in Chapter|\\(|\\n|$)\", response)\n        if text_mentions and len(set([t.strip().lower() for t in text_mentions if len(t.strip()) > 3])) == 1:\n            confidence += 0.1\n            \n        # 3. Presence of Sanskrit terms\n        sanskrit_term_count = len(re.findall(r'\\b[A-Za-z]+a\\b', response))  # Many Sanskrit words end in 'a'\n        confidence += min(0.05, sanskrit_term_count * 0.01)\n        \n        # 4. Citations or references\n        if re.search(r\"according to|traditionally attributed|scholars agree\", response, re.IGNORECASE):\n            confidence += 0.05\n            \n        # 5. Language suggesting uncertainty (negative factor)\n        if re.search(r\"might be|possibly|perhaps|I think|probably|could be\", response, re.IGNORECASE):\n            confidence -= 0.1\n            \n        # Ensure confidence is between 0 and 1\n        return max(0.1, min(0.99, confidence))\n    \n    def update(self, reward, learning_rate=0.01):\n        \"\"\"Update the model based on reward signal\"\"\"\n        # In a real implementation, this would fine-tune the model\n        # For demonstration purposes, we'll just log the reward\n        self.metrics[\"reward_history\"].append(reward)\n        logger.info(f\"Model received reward: {reward:.4f}\")\n        \n    def update_from_memory(self, quote, prediction, reward, true_source):\n        \"\"\"Learn from stored memory examples\"\"\"\n        # This would involve gradient updates in a real implementation\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:54:52.820270Z","iopub.execute_input":"2025-05-24T15:54:52.820811Z","iopub.status.idle":"2025-05-24T15:54:52.839709Z","shell.execute_reply.started":"2025-05-24T15:54:52.820790Z","shell.execute_reply":"2025-05-24T15:54:52.838958Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class PredictionResult:\n    def __init__(self, text, chapter, verse, genre=\"unknown\", confidence=0.0):\n        self.text = text\n        self.chapter = chapter\n        self.verse = verse\n        self.genre = genre\n        self.confidence = confidence\n    \n    def exact_match(self, other):\n        return (self.text.lower() == other.text.lower() and \n                self.chapter == other.chapter and \n                self.verse == other.verse)\n    \n    def text_match(self, other):\n        return self.text.lower() == other.text.lower()\n    \n    def __str__(self):\n        return f\"{self.text}, Chapter {self.chapter}, Verse {self.verse} (Genre: {self.genre}, Confidence: {self.confidence:.2f})\"\n\nclass SourceInfo:\n    def __init__(self, text, chapter, verse, genre, content, language=\"sanskrit\"):\n        self.text = text\n        self.chapter = chapter\n        self.verse = verse\n        self.genre = genre\n        self.content = content\n        self.language = language\n\nclass SuperSophisticatedRewardFunction:\n    def __init__(self):\n        # Initialize sophisticated components\n        self.sanskrit_embedding_model = self.load_sanskrit_embedding_model()\n        self.sanskrit_meter_analyzer = self.load_meter_analyzer()\n        self.cultural_context_db = self.load_cultural_context()\n        \n        # Historical performance tracking\n        self.performance_history = []\n        \n        # Weights for different reward components\n        self.weights = {\n            \"exact_match\": 0.25,\n            \"semantic_similarity\": 0.20,\n            \"stylistic_recognition\": 0.15,\n            \"contextual_understanding\": 0.15,\n            \"confidence_calibration\": 0.10,\n            \"historical_consistency\": 0.10,\n            \"identification_speed\": 0.05\n        }\n        \n    def load_sanskrit_embedding_model(self):\n        # Placeholder - would load a model specifically tuned for Sanskrit\n        return None\n        \n    def load_meter_analyzer(self):\n        # Placeholder - would load a system to analyze Sanskrit meters\n        return None\n        \n    def load_cultural_context(self):\n        # Placeholder - would load cultural/historical context database\n        return None\n    \n    def calculate_reward(self, prediction, true_source, response_time, candidates):\n        \"\"\"\n        Sophisticated reward function with multiple dimensions of evaluation\n        \"\"\"\n        reward_components = {}\n        \n        # 1. Exact Match Component (hierarchical)\n        if prediction.exact_match(true_source):\n            reward_components[\"exact_match\"] = 1.0\n        elif prediction.text_match(true_source):\n            if abs(prediction.chapter - true_source.chapter) <= 1:\n                reward_components[\"exact_match\"] = 0.7  # Right text, close chapter\n            else:\n                reward_components[\"exact_match\"] = 0.5  # Right text, wrong chapter\n        elif prediction.genre == true_source.genre:\n            reward_components[\"exact_match\"] = 0.2  # Right genre at least\n        else:\n            reward_components[\"exact_match\"] = 0.0  # Completely wrong\n        \n        # 2. Semantic Similarity Component\n        semantic_sim = self.calculate_semantic_similarity(prediction, true_source)\n        reward_components[\"semantic_similarity\"] = semantic_sim\n        \n        # 3. Stylistic Recognition Component\n        style_score = self.evaluate_stylistic_recognition(prediction, true_source)\n        reward_components[\"stylistic_recognition\"] = style_score\n        \n        # 4. Contextual Understanding Component\n        context_score = self.evaluate_contextual_understanding(prediction, true_source)\n        reward_components[\"contextual_understanding\"] = context_score\n        \n        # 5. Confidence Calibration Component\n        confidence_score = self.evaluate_confidence_calibration(prediction, true_source, candidates)\n        reward_components[\"confidence_calibration\"] = confidence_score\n        \n        # 6. Historical Consistency Component\n        history_score = self.evaluate_historical_consistency(prediction)\n        reward_components[\"historical_consistency\"] = history_score\n        \n        # 7. Speed Component (faster is better, but with diminishing returns)\n        speed_score = min(1.0, 5.0/max(1.0, response_time))  # 5 seconds is optimal\n        reward_components[\"identification_speed\"] = speed_score\n        \n        # Calculate weighted sum\n        final_reward = sum(self.weights[k] * v for k, v in reward_components.items())\n        \n        # Apply bonus for exceptional performance\n        if (reward_components[\"exact_match\"] > 0.9 and \n            reward_components[\"semantic_similarity\"] > 0.8 and\n            reward_components[\"confidence_calibration\"] > 0.7):\n            final_reward *= 1.2  # 20% bonus for excellent performance\n            \n        # Apply penalty for egregiously wrong answers with high confidence\n        if (reward_components[\"exact_match\"] < 0.1 and \n            prediction.confidence > 0.8):\n            final_reward *= 0.5  # 50% penalty for confidently wrong answers\n            \n        # Log detailed reward breakdown\n        logger.info(f\"Reward breakdown: {json.dumps(reward_components, indent=2)}\")\n        \n        # Update history\n        self.performance_history.append({\n            \"reward\": final_reward,\n            \"components\": reward_components\n        })\n        \n        return final_reward\n    \n    def calculate_semantic_similarity(self, prediction, true_source):\n        \"\"\"Calculate semantic similarity between prediction and true source\"\"\"\n        # In a real implementation, this would use embeddings from Sanskrit texts\n        \n        # Simulate semantic similarity based on text, chapter, genre\n        text_match = prediction.text.lower() == true_source.text.lower()\n        chapter_diff = abs(prediction.chapter - true_source.chapter) if prediction.chapter and true_source.chapter else 999\n        genre_match = prediction.genre.lower() == true_source.genre.lower()\n        \n        # Calculate similarity score\n        sim_score = 0.0\n        if text_match:\n            sim_score += 0.6\n            if chapter_diff <= 2:\n                sim_score += 0.3 * (1 - (chapter_diff / 5))\n        elif genre_match:\n            sim_score += 0.3\n            \n        # Add random noise to simulate actual semantic similarity calculation\n        sim_score += random.uniform(-0.1, 0.1)\n        \n        return max(0.0, min(1.0, sim_score))\n    \n    def evaluate_stylistic_recognition(self, prediction, true_source):\n        \"\"\"Evaluate how well the model recognized stylistic elements\"\"\"\n        # In a real implementation, would analyze metrics, rhetorical devices, etc.\n        \n        # Simulate stylistic recognition score\n        base_score = 0.0\n        \n        if prediction.text_match(true_source):\n            base_score += 0.7  # Basic recognition of text style\n            \n            # Additional points for verse structure recognition\n            if prediction.verse and true_source.verse:\n                verse_diff = abs(prediction.verse - true_source.verse)\n                if verse_diff <= 5:\n                    base_score += 0.2 * (1 - (verse_diff / 10))\n        \n        elif prediction.genre == true_source.genre:\n            base_score += 0.4  # Recognized genre style at least\n            \n        # Add slight randomness\n        style_score = base_score + random.uniform(-0.05, 0.05)\n        return max(0.0, min(1.0, style_score))\n    \n    def evaluate_contextual_understanding(self, prediction, true_source):\n        \"\"\"Evaluate understanding of cultural/philosophical context\"\"\"\n        # Simplified simulation - would be much more complex in reality\n        \n        if prediction.text_match(true_source):\n            # Strong contextual understanding if text is correct\n            return random.uniform(0.7, 0.9)\n        elif prediction.genre == true_source.genre:\n            # Moderate understanding if genre is correct\n            base_score = random.uniform(0.4, 0.6)\n            \n            # Adjust based on temporal/historical proximity of texts\n            # If predicted text is from same historical period as true text\n            if self.are_historically_related(prediction.text, true_source.text):\n                base_score += 0.1\n                \n            # Adjust based on philosophical school alignment\n            if self.share_philosophical_framework(prediction.text, true_source.text):\n                base_score += 0.15\n                \n            return min(0.8, base_score)  # Cap at 0.8 for non-exact matches\n        else:\n            # Poor contextual understanding - texts from different traditions\n            base_score = random.uniform(0.1, 0.3)\n            \n            # Check if there's any redeeming recognition of broader context\n            if self.has_contextual_overlap(prediction.text, true_source.text):\n                base_score += 0.2\n                \n            # Check if explanation shows awareness of cultural context despite wrong attribution\n            if prediction.confidence < 0.6:  # Model showed appropriate uncertainty\n                base_score += 0.1\n                \n            return min(0.4, base_score)  # Cap at 0.4 for completely different traditions\n        \n    def are_historically_related(self, text1, text2):\n        \"\"\"Check if two texts belong to the same historical period\"\"\"\n        # Historical period database (simplified)\n        historical_periods = {\n            \"Early Vedic\": [\"Rigveda\", \"Samaveda\", \"Yajurveda\", \"Atharvaveda\"],\n            \"Late Vedic\": [\"Brahmanas\", \"Aranyakas\", \"Early Upanishads\"],\n            \"Epic Period\": [\"Mahabharata\", \"Ramayana\", \"Bhagavad Gita\", \"Harivamsa\"],\n            \"Classical Sanskrit\": [\"Kalidasa works\", \"Bhasa plays\", \"Mricchakatika\"],\n            \"Puranic Period\": [\"Puranas\", \"Later Upanishads\", \"Tantric texts\"],\n            \"Shastra Period\": [\"Arthashastra\", \"Kamasutra\", \"Natyashastra\"],\n            \"Philosophical Schools\": [\"Yoga Sutras\", \"Nyaya Sutras\", \"Vaisheshika Sutras\", \n                                    \"Samkhya Karika\", \"Brahma Sutras\", \"Buddhist texts\"]\n        }\n        \n        # Find periods for each text\n        text1_periods = []\n        text2_periods = []\n        \n        for period, texts in historical_periods.items():\n            for t in texts:\n                if t.lower() in text1.lower():\n                    text1_periods.append(period)\n                if t.lower() in text2.lower():\n                    text2_periods.append(period)\n        \n        # Check for overlap in periods\n        return bool(set(text1_periods).intersection(set(text2_periods)))\n    \n    def share_philosophical_framework(self, text1, text2):\n        \"\"\"Check if two texts share philosophical framework\"\"\"\n        # Philosophical schools and their associated texts\n        philosophical_schools = {\n            \"Vedanta\": [\"Upanishads\", \"Brahma Sutras\", \"Bhagavad Gita\", \"Advaita\"],\n            \"Yoga\": [\"Yoga Sutras\", \"Hatha Yoga\", \"Patanjali\"],\n            \"Nyaya\": [\"Nyaya Sutras\", \"Vatsyayana\", \"Uddyotakara\"],\n            \"Vaisheshika\": [\"Vaisheshika Sutras\", \"Prashastapada\"],\n            \"Samkhya\": [\"Samkhya Karika\", \"Samkhya Sutras\"],\n            \"Mimamsa\": [\"Mimamsa Sutras\", \"Shabara Bhashya\"],\n            \"Buddhist\": [\"Madhyamaka\", \"Yogacara\", \"Abhidharma\", \"Pramana\"],\n            \"Jaina\": [\"Jaina Agamas\", \"Anekantavada\"],\n            \"Tantric\": [\"Tantras\", \"Agamas\", \"Shakta\"]\n        }\n        \n        # Find schools for each text\n        text1_schools = []\n        text2_schools = []\n        \n        for school, keywords in philosophical_schools.items():\n            if any(kw.lower() in text1.lower() for kw in keywords):\n                text1_schools.append(school)\n            if any(kw.lower() in text2.lower() for kw in keywords):\n                text2_schools.append(school)\n        \n        # Check for overlap\n        return bool(set(text1_schools).intersection(set(text2_schools)))\n    \n    def has_contextual_overlap(self, text1, text2):\n        \"\"\"Check if there's any cultural/contextual overlap despite different traditions\"\"\"\n        # Cultural contexts that might overlap\n        context_categories = {\n            \"Devotional\": [\"bhakti\", \"devotion\", \"prayer\", \"worship\", \"devotional\"],\n            \"Ethical\": [\"dharma\", \"duty\", \"ethics\", \"moral\", \"righteousness\"],\n            \"Cosmological\": [\"creation\", \"universe\", \"cosmos\", \"time cycles\", \"yugas\"],\n            \"Metaphysical\": [\"atman\", \"brahman\", \"reality\", \"consciousness\", \"existence\"],\n            \"Ritual\": [\"yajna\", \"ritual\", \"ceremony\", \"sacrifice\", \"puja\"],\n            \"Yogic\": [\"meditation\", \"concentration\", \"samadhi\", \"practice\"]\n        }\n        \n        # Perform actual keyword matching to detect contextual overlap\n        text1_lower = text1.lower()\n        text2_lower = text2.lower()\n        \n        # Track which categories each text belongs to\n        text1_categories = set()\n        text2_categories = set()\n        \n        for category, keywords in context_categories.items():\n            if any(keyword in text1_lower for keyword in keywords):\n                text1_categories.add(category)\n            if any(keyword in text2_lower for keyword in keywords):\n                text2_categories.add(category)\n        \n        # Calculate overlap score based on shared categories\n        shared_categories = text1_categories.intersection(text2_categories)\n        \n        if not shared_categories:\n            # No categorical overlap detected\n            return False\n        \n        # Calculate overlap strength based on number of shared categories\n        overlap_strength = len(shared_categories) / len(context_categories)\n        \n        # Apply sophisticated weighting based on category importance\n        # Some categories indicate stronger overlap than others\n        category_weights = {\n            \"Metaphysical\": 1.5,  # Strongest indicator of shared worldview\n            \"Ethical\": 1.3,       # Strong indicator of shared values\n            \"Cosmological\": 1.2,  # Important shared background\n            \"Devotional\": 1.1,    # Significant but can vary widely\n            \"Ritual\": 0.9,        # May be superficially similar\n            \"Yogic\": 1.0          # Standard weight\n        }\n        \n        # Apply weights to calculate final overlap score\n        weighted_score = sum(category_weights.get(cat, 1.0) for cat in shared_categories) / len(shared_categories)\n        final_overlap_score = overlap_strength * weighted_score\n        \n        # Additional boost if texts share specific technical terminology\n        sanskrit_technical_terms = [\n            \"tattva\", \"guna\", \"purusha\", \"prakriti\", \"maya\", \"samsara\",\n            \"karma\", \"moksha\", \"satchitananda\", \"vedanta\", \"darshana\"\n        ]\n        \n        shared_terms = 0\n        for term in sanskrit_technical_terms:\n            if term in text1_lower and term in text2_lower:\n                shared_terms += 1\n        \n        if shared_terms > 0:\n            terminology_boost = 0.1 * min(shared_terms, 5)  # Cap at 0.5 boost\n            final_overlap_score += terminology_boost\n        \n        # Check for time period references that might indicate contextual similarity\n        time_period_indicators = {\n            \"vedic\": \"ancient\",\n            \"classical\": \"classical\",\n            \"medieval\": \"medieval\",\n            \"modern\": \"modern\"\n        }\n        \n        for indicator, period in time_period_indicators.items():\n            if indicator in text1_lower and indicator in text2_lower:\n                final_overlap_score += 0.1  # Temporal context overlap\n        \n        # Normalize score to 0-1 range\n        final_overlap_score = min(1.0, final_overlap_score)\n        \n        # Threshold for significant overlap (more than random chance)\n        return final_overlap_score > 0.3\n    \n    \n    def evaluate_confidence_calibration(self, prediction, true_source, candidates):\n        \"\"\"Evaluate how well the model's confidence matches its accuracy\"\"\"\n        # Perfect calibration: confidence = probability of being correct\n        \n        # Calculate actual correctness\n        if prediction.exact_match(true_source):\n            correctness = 1.0\n        elif prediction.text_match(true_source):\n            correctness = 0.7\n        elif prediction.genre == true_source.genre:\n            correctness = 0.3\n        else:\n            correctness = 0.0\n        \n        # Calculate calibration error (difference between confidence and correctness)\n        calibration_error = abs(prediction.confidence - correctness)\n        \n        # Convert error to score (lower error = higher score)\n        calibration_score = max(0.0, 1.0 - calibration_error * 2)\n        \n        # Bonus for having appropriate confidence interval across candidates\n        if len(candidates) > 1:\n            # Calculate confidence spread\n            confidence_values = [c[\"confidence\"] for c in candidates]\n            confidence_range = max(confidence_values) - min(confidence_values)\n            \n            # Good calibration should have meaningful differences in confidence\n            if 0.2 <= confidence_range <= 0.6:\n                calibration_score += 0.1\n                \n            # Check if confidence ordering matches correctness ordering\n            # Sort candidates by confidence\n            sorted_candidates = sorted(candidates, key=lambda x: x[\"confidence\"], reverse=True)\n            correctness_values = []\n            \n            for cand in sorted_candidates:\n                pred = cand[\"prediction\"]\n                if pred.exact_match(true_source):\n                    correctness_values.append(1.0)\n                elif pred.text_match(true_source):\n                    correctness_values.append(0.7)\n                elif pred.genre == true_source.genre:\n                    correctness_values.append(0.3)\n                else:\n                    correctness_values.append(0.0)\n                    \n            # Check if correctness decreases as confidence decreases\n            is_monotonic = all(correctness_values[i] >= correctness_values[i+1] \n                               for i in range(len(correctness_values)-1))\n            \n            if is_monotonic:\n                calibration_score += 0.1\n        \n        # Penalize overconfidence more than underconfidence\n        if prediction.confidence > correctness:\n            overconfidence_penalty = (prediction.confidence - correctness) * 0.3\n            calibration_score -= overconfidence_penalty\n        \n        return max(0.0, min(1.0, calibration_score))\n    \n    def evaluate_historical_consistency(self, prediction):\n        \"\"\"Evaluate consistency with historical understanding of text relationships\"\"\"\n        # Dictionary of historically related texts\n        historical_relationships = {\n            \"Bhagavad Gita\": [\"Mahabharata\", \"Puranas\", \"Vedanta Sutras\"],\n            \"Ramayana\": [\"Puranas\", \"Mahabharata\"],\n            \"Yoga Sutras\": [\"Samkhya Karika\", \"Nyaya Sutras\", \"Brahma Sutras\"],\n            \"Upanishads\": [\"Vedas\", \"Brahma Sutras\", \"Bhagavad Gita\"],\n            \"Vedas\": [\"Brahmanas\", \"Upanishads\", \"Aranyakas\"],\n            \"Puranas\": [\"Mahabharata\", \"Ramayana\", \"Bhagavad Gita\"],\n            \"Brahma Sutras\": [\"Upanishads\", \"Bhagavad Gita\", \"Vedas\"],\n            \"Arthashastra\": [\"Nitishastra\", \"Dharmashastra\"],\n            \"Kamasutra\": [\"Nitishastra\", \"Natyashastra\"],\n            \"Natyashastra\": [\"Kamasutra\", \"Kavyashastra\"]\n        }\n        \n        # Check if this text has historically consistent relationships\n        for text, related_texts in historical_relationships.items():\n            if text.lower() in prediction.text.lower():\n                # The prediction is consistent with historical knowledge\n                # Calculate a score based on confidence and specificity\n                base_score = 0.7\n                \n                # Adjust for specificity (chapter and verse identification)\n                if prediction.chapter and prediction.verse:\n                    base_score += 0.2\n                elif prediction.chapter:\n                    base_score += 0.1\n                    \n                # Adjust slightly for randomness\n                random_adjustment = random.uniform(-0.05, 0.05)\n                final_score = base_score + random_adjustment\n                \n                return max(0.0, min(1.0, final_score))\n        \n        # If the text isn't in our historical relationships database\n        # Give a moderate score with more randomness\n        base_score = 0.5\n        random_adjustment = random.uniform(-0.1, 0.1)\n        return max(0.0, min(1.0, base_score + random_adjustment))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:25:27.272952Z","iopub.execute_input":"2025-05-24T15:25:27.273627Z","iopub.status.idle":"2025-05-24T15:25:27.691397Z","shell.execute_reply.started":"2025-05-24T15:25:27.273601Z","shell.execute_reply":"2025-05-24T15:25:27.690717Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I've reviewed the code and found several issues that need correction:\n\n# 1. **Indentation Issues**: The functions are indented incorrectly, suggesting they might be inside a class when they should be at the global level.\n\n# 2. **Function Reference Issue**: The `generate_performance_report` function is called in `evaluate_performance` but it's defined as a method within the `SuperSophisticatedRewardFunction` class.\n\n# 3. **Confidence Calibration Error**: There's an error in the confidence calibration calculation where it tries to access `metrics[\"all_predictions\"]` which isn't defined.\n\n# Here's the corrected code:\n\n# ```python\ndef sample_from_gretil(corpus_path=\"gretil_corpus/\", training_progress=0.5):\n    \"\"\"Sample a quote from the GRETIL corpus with metadata\"\"\"\n    # In a real implementation, this would access actual GRETIL files\n    \n    # Simulate different Sanskrit texts\n    text_samples = [\n        {\n            \"text\": \"Bhagavad Gita\",\n            \"chapter\": 2,\n            \"verse\": 47,\n            \"content\": \"कर्मण्येवाधिकारस्ते मा फलेषु कदाचन। मा कर्मफलहेतुर्भूर्मा ते सङ्गोऽस्त्वकर्मणि॥\",\n            \"translation\": \"You have a right to perform your prescribed duties, but you are not entitled to the fruits of your actions.\",\n            \"genre\": \"Philosophy\"\n        },\n        {\n            \"text\": \"Ramayana\",\n            \"chapter\": 1,\n            \"verse\": 3,\n            \"content\": \"कः नु अस्मिन् साम्प्रतं लोके गुणवान् कः च वीर्यवान्। धर्मज्ञः च कृतज्ञः च सत्यवाक्यो दृढव्रतः॥\",\n            \"translation\": \"Who in this world now is virtuous? Who is heroic? Who knows dharma? Who is grateful? Who speaks truth and is firm in his vows?\",\n            \"genre\": \"Epic\"\n        },\n        {\n            \"text\": \"Yoga Sutras\",\n            \"chapter\": 1,\n            \"verse\": 2,\n            \"content\": \"योगश्चित्तवृत्तिनिरोधः॥\",\n            \"translation\": \"Yoga is the cessation of the modifications of the mind.\",\n            \"genre\": \"Philosophy\"\n        },\n        {\n            \"text\": \"Mahabharata\",\n            \"chapter\": 6,\n            \"verse\": 40,\n            \"content\": \"यदा यदा हि धर्मस्य ग्लानिर्भवति भारत। अभ्युत्थानमधर्मस्य तदात्मानं सृजाम्यहम्॥\",\n            \"translation\": \"Whenever there is a decline of righteousness and rise of unrighteousness, I manifest myself.\",\n            \"genre\": \"Epic\"\n        },\n        {\n            \"text\": \"Arthashastra\",\n            \"chapter\": 3,\n            \"verse\": 12,\n            \"content\": \"सुखस्य मूलं धर्मः। धर्मस्य मूलं अर्थः। अर्थस्य मूलं राज्यं। राज्यस्य मूलं इन्द्रिय जयः॥\",\n            \"translation\": \"The root of happiness is dharma. The root of dharma is artha. The root of artha is governance. The root of governance is self-control.\",\n            \"genre\": \"Technical\"\n        },\n        {\n            \"text\": \"Upanishads\",\n            \"chapter\": 6,\n            \"verse\": 8,\n            \"content\": \"तत्त्वमसि॥\",\n            \"translation\": \"Thou art That.\",\n            \"genre\": \"Philosophy\"\n        },\n        {\n            \"text\": \"Kamasutra\",\n            \"chapter\": 2,\n            \"verse\": 5,\n            \"content\": \"धर्मार्थकामानां यस्यैकोऽपि न विद्यते। जालं केवलनिःश्वासैः शुष्यत्याकाशमत्स्यवत्॥\",\n            \"translation\": \"One who has neither dharma, artha nor kama, dries up like a fish out of water, just by breathing.\",\n            \"genre\": \"Technical\"\n        }\n    ]\n    \n    # Add difficulty level based on training progress\n    if training_progress < 0.3:\n        # Early training: sample from more distinctive texts\n        sample = random.choice(text_samples[:4])  # More iconic texts\n    elif training_progress < 0.7:\n        # Mid training: sample from all texts\n        sample = random.choice(text_samples)\n    else:\n        # Late training: include more challenging variations\n        sample = random.choice(text_samples)\n        # Sometimes modify the quote to make it more challenging\n        if random.random() < 0.3:\n            words = sample[\"content\"].split()\n            # Remove or shuffle some words to simulate fragmentary quotes\n            if len(words) > 4 and random.random() < 0.5:\n                num_to_remove = random.randint(1, min(3, len(words)//3))\n                for _ in range(num_to_remove):\n                    idx = random.randint(0, len(words)-1)\n                    words.pop(idx)\n            sample[\"content\"] = \" \".join(words)\n    \n    # Create source info object\n    source = SourceInfo(\n        text=sample[\"text\"],\n        chapter=sample[\"chapter\"],\n        verse=sample[\"verse\"],\n        genre=sample[\"genre\"],\n        content=sample[\"content\"]\n    )\n    \n    # Return the Sanskrit content and source info\n    return sample[\"content\"], source\n\ndef generate_performance_report(metrics, confusion_data, reward_function, output_path):\n    \"\"\"Generate a comprehensive performance report as PDF\"\"\"\n    # This would use a PDF generation library like reportlab\n    # Simplified version just logs detailed metrics\n    logger.info(\"Generating performance report...\")\n    \n    # Overall performance summary\n    logger.info(\"Overall Performance:\")\n    logger.info(f\"  Exact match accuracy: {metrics['exact_match']:.4f}\")\n    logger.info(f\"  Text match accuracy: {metrics['text_match']:.4f}\")\n    logger.info(f\"  Genre match accuracy: {metrics['genre_match']:.4f}\")\n    logger.info(f\"  Overall F1 score: {metrics['overall_f1']:.4f}\")\n    \n    # Per-text performance\n    logger.info(\"Performance by Text:\")\n    for text, text_metrics in metrics[\"per_text_metrics\"].items():\n        logger.info(f\"  {text}:\")\n        logger.info(f\"    Precision: {text_metrics['precision']:.4f}\")\n        logger.info(f\"    Recall: {text_metrics['recall']:.4f}\")\n        logger.info(f\"    F1: {text_metrics['f1']:.4f}\")\n    \n    # Reward component analysis\n    logger.info(\"Reward Component Analysis:\")\n    component_averages = {}\n    for history in reward_function.performance_history:\n        for component, value in history[\"components\"].items():\n            if component not in component_averages:\n                component_averages[component] = []\n            component_averages[component].append(value)\n    \n    for component, values in component_averages.items():\n        logger.info(f\"  {component}: {sum(values)/len(values):.4f}\")\n    \n    # Log calibration information\n    logger.info(f\"Expected Calibration Error: {metrics.get('expected_calibration_error', 'N/A')}\")\n    \n    # In a real implementation, this would create a PDF with charts, tables, etc.\n    logger.info(f\"Report saved to {output_path}\")\n\ndef evaluate_performance(model, num_samples=100):\n    \"\"\"Evaluate model performance across a range of metrics\"\"\"\n    metrics = {\n        \"exact_match\": 0,\n        \"text_match\": 0,\n        \"genre_match\": 0,\n        \"average_reward\": 0.0,\n        \"average_confidence\": 0.0,\n        \"confidence_calibration\": 0.0,\n        \"response_time\": [],\n        \"all_predictions\": []  # Added this to store prediction data\n    }\n    \n    confusion_matrix_data = {\n        \"true\": [],\n        \"predicted\": []\n    }\n    \n    reward_function = SuperSophisticatedRewardFunction()\n    \n    logger.info(f\"Starting performance evaluation with {num_samples} samples...\")\n    \n    for i in tqdm(range(num_samples)):\n        quote, true_source = sample_from_gretil()\n        \n        start_time = time.time()\n        prediction, candidates = model.identify_source(quote)\n        response_time = time.time() - start_time\n        \n        # Record basic metrics\n        is_correct = prediction.exact_match(true_source)\n        \n        # Store prediction data for calibration analysis\n        metrics[\"all_predictions\"].append({\n            \"confidence\": prediction.confidence,\n            \"is_correct\": is_correct\n        })\n        \n        if is_correct:\n            metrics[\"exact_match\"] += 1\n        if prediction.text_match(true_source):\n            metrics[\"text_match\"] += 1\n        if prediction.genre == true_source.genre:\n            metrics[\"genre_match\"] += 1\n            \n        # Calculate reward\n        reward = reward_function.calculate_reward(prediction, true_source, response_time, candidates)\n        metrics[\"average_reward\"] += reward\n        \n        # Record confidence\n        metrics[\"average_confidence\"] += prediction.confidence\n        \n        # Record response time\n        metrics[\"response_time\"].append(response_time)\n        \n        # For confusion matrix\n        confusion_matrix_data[\"true\"].append(true_source.text)\n        confusion_matrix_data[\"predicted\"].append(prediction.text)\n    \n    # Calculate averages\n    metrics[\"exact_match\"] /= num_samples\n    metrics[\"text_match\"] /= num_samples\n    metrics[\"genre_match\"] /= num_samples\n    metrics[\"average_reward\"] /= num_samples\n    metrics[\"average_confidence\"] /= num_samples\n    metrics[\"avg_response_time\"] = sum(metrics[\"response_time\"]) / len(metrics[\"response_time\"])\n    \n    # Generate confusion matrix\n    unique_texts = list(set(confusion_matrix_data[\"true\"] + confusion_matrix_data[\"predicted\"]))\n    text_to_idx = {text: i for i, text in enumerate(unique_texts)}\n    \n    y_true = [text_to_idx[t] for t in confusion_matrix_data[\"true\"]]\n    y_pred = [text_to_idx[t] for t in confusion_matrix_data[\"predicted\"]]\n    \n    cm = confusion_matrix(y_true, y_pred, labels=range(len(unique_texts)))\n    \n    # Calculate precision, recall, F1 per text\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, labels=range(len(unique_texts)), average=None\n    )\n    \n    # Add to metrics\n    metrics[\"per_text_metrics\"] = {\n        unique_texts[i]: {\n            \"precision\": precision[i],\n            \"recall\": recall[i],\n            \"f1\": f1[i]\n        } for i in range(len(unique_texts))\n    }\n    \n    # Overall F1\n    metrics[\"overall_f1\"] = f1_score(y_true, y_pred, average='weighted')\n    \n    # Generate detailed performance report\n    report_path = f\"performance_report_{time.strftime('%Y%m%d_%H%M%S')}.pdf\"\n    generate_performance_report(metrics, confusion_matrix_data, reward_function, report_path)\n    \n    # Generate visualizations\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', xticklabels=unique_texts, yticklabels=unique_texts)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png')\n    \n    # Generate reward history plot\n    plt.figure(figsize=(10, 6))\n    rewards_plot = [h[\"reward\"] for h in reward_function.performance_history[-100:]]\n    plt.plot(rewards_plot)\n    plt.xlabel('Sample')\n    plt.ylabel('Reward')\n    plt.title('Reward History (Last 100 Samples)')\n    plt.tight_layout()\n    plt.savefig('reward_history.png')\n    \n    # Generate confidence calibration plot\n    plt.figure(figsize=(10, 6))\n    # Create calibration curve\n    confidence_bins = np.linspace(0, 1, 11)\n    bin_accuracies = [0] * 10\n    bin_counts = [0] * 10\n    \n    # Collect all predictions and their actual correctness for calibration analysis\n    all_confidences = []\n    all_correctness = []\n    \n    for i in range(num_samples):\n        confidence = metrics[\"all_predictions\"][i][\"confidence\"]\n        is_correct = metrics[\"all_predictions\"][i][\"is_correct\"]\n        \n        all_confidences.append(confidence)\n        all_correctness.append(1.0 if is_correct else 0.0)\n        \n        # Add to appropriate bin\n        bin_idx = min(9, int(confidence * 10))\n        bin_counts[bin_idx] += 1\n        if is_correct:\n            bin_accuracies[bin_idx] += 1\n    \n    # Calculate accuracy per bin\n    for i in range(10):\n        if bin_counts[i] > 0:\n            bin_accuracies[i] /= bin_counts[i]\n    \n    # Plot calibration curve\n    plt.plot([0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95], \n             bin_accuracies, 'bo-', label='Model calibration')\n    plt.plot([0, 1], [0, 1], 'r--', label='Perfect calibration')\n    \n    # Add shaded area for confidence intervals\n    conf_lower = []\n    conf_upper = []\n    for i in range(10):\n        if bin_counts[i] >= 5:\n            # Wilson score interval for binomial proportion confidence interval\n            n = bin_counts[i]\n            p = bin_accuracies[i]\n            z = 1.96  # 95% confidence\n            denominator = 1 + z**2/n\n            center = (p + z**2/(2*n)) / denominator\n            plusminus = z * np.sqrt(p*(1-p)/n + z**2/(4*n**2)) / denominator\n            conf_lower.append(max(0, center - plusminus))\n            conf_upper.append(min(1, center + plusminus))\n        else:\n            # Not enough samples for reliable interval\n            conf_lower.append(None)\n            conf_upper.append(None)\n    \n    # Filter out None values\n    x_vals = []\n    lower_vals = []\n    upper_vals = []\n    for i in range(10):\n        if conf_lower[i] is not None:\n            x_vals.append(0.05 + i*0.1)\n            lower_vals.append(conf_lower[i])\n            upper_vals.append(conf_upper[i])\n    \n    # Add confidence interval if we have enough data points\n    if len(x_vals) > 1:\n        plt.fill_between(x_vals, lower_vals, upper_vals, alpha=0.2, color='blue')\n    \n    # Customize plot\n    plt.xlabel('Confidence')\n    plt.ylabel('Accuracy')\n    plt.title('Confidence Calibration Curve')\n    plt.legend(loc='lower right')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.savefig('confidence_calibration.png')\n    \n    # Calculate calibration metrics\n    ece = 0  # Expected Calibration Error\n    for i in range(10):\n        if bin_counts[i] > 0:\n            ece += (bin_counts[i] / num_samples) * abs(bin_accuracies[i] - (0.05 + i*0.1))\n    metrics[\"expected_calibration_error\"] = ece\n    \n\n    # Generate reliability diagram\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    width = 0.08\n    for i in range(10):\n        plt.bar(0.05 + i*0.1, bin_accuracies[i], width=width, alpha=0.8, edgecolor='black')\n        plt.bar(0.05 + i*0.1, 0.05 + i*0.1, width=width, alpha=0.2, edgecolor='red')\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('Confidence')\n    plt.ylabel('Accuracy')\n    plt.title('Reliability Diagram')\n    plt.grid(True, alpha=0.3)\n    \n    # Histogram of prediction confidences\n    plt.subplot(1, 2, 2)\n    plt.hist(all_confidences, bins=10, alpha=0.5, edgecolor='black')\n    plt.xlabel('Confidence')\n    plt.ylabel('Count')\n    plt.title('Confidence Distribution')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig('reliability_diagram.png')\n    \n    return metrics\n\ndef train_sanskrit_librarian(episodes=5000, evaluation_interval=500):\n    \"\"\"Train the Sanskrit Librarian model using RL\"\"\"\n    model = QwenSanskritLibrarian(model_name=\"Qwen/Qwen3-0.6B\")\n    reward_function = SuperSophisticatedRewardFunction()\n    memory_buffer = []\n    \n    # Training parameters\n    learning_rate = 0.01\n    buffer_size = 1000\n    batch_size = 32\n    \n    # Learning rate schedule - decrease over time\n    def get_learning_rate(episode):\n        return max(0.001, learning_rate * (1 - episode/episodes))\n    \n    # Performance tracking\n    rewards = []\n    accuracies = []\n    \n    # Initialize tracker for best model\n    best_performance = 0\n    best_model_path = None\n    \n    logger.info(f\"Starting training for {episodes} episodes...\")\n    \n    for episode in tqdm(range(episodes)):\n        # Sample quote from GRETIL\n        quote, true_source = sample_from_gretil(training_progress=episode/episodes)\n        \n        # Get model's prediction\n        start_time = time.time()\n        prediction, candidates = model.identify_source(quote)\n        response_time = time.time() - start_time\n        \n        # Calculate reward\n        reward = reward_function.calculate_reward(prediction, true_source, response_time, candidates)\n        rewards.append(reward)\n        \n        # Update model based on reward with current learning rate\n        current_lr = get_learning_rate(episode)\n        model.update(reward, current_lr)\n        \n        # Store experience in buffer\n        memory_buffer.append((quote, prediction, reward, true_source))\n        if len(memory_buffer) > buffer_size:\n            memory_buffer.pop(0)  # Remove oldest experience\n        \n        # Experience replay\n        if episode % 10 == 0 and len(memory_buffer) >= batch_size:\n            batch = random.sample(memory_buffer, batch_size)\n            for q, p, r, t in batch:\n                model.update_from_memory(q, p, r, t)\n        \n        # Periodically log progress\n        if episode % 100 == 0:\n            avg_reward = sum(rewards[-100:]) / min(100, len(rewards))\n            logger.info(f\"Episode {episode}: Average reward = {avg_reward:.4f}, Learning rate = {current_lr:.6f}\")\n        \n        # Evaluate performance\n        if episode % evaluation_interval == 0:\n            logger.info(f\"Episode {episode}: Evaluating performance...\")\n            metrics = evaluate_performance(model, num_samples=50)\n            accuracies.append(metrics[\"text_match\"])\n            \n            # Save model if it's the best so far\n            if metrics[\"text_match\"] > best_performance:\n                best_performance = metrics[\"text_match\"]\n                best_model_path = f\"best_model_episode_{episode}.pt\"\n                logger.info(f\"New best model with text accuracy {best_performance:.4f}\")\n                \n            # Log detailed metrics\n            logger.info(f\"Episode {episode} metrics:\")\n            logger.info(f\"  Exact match accuracy: {metrics['exact_match']:.4f}\")\n            logger.info(f\"  Text match accuracy: {metrics['text_match']:.4f}\")\n            logger.info(f\"  Genre match accuracy: {metrics['genre_match']:.4f}\")\n            logger.info(f\"  Average reward: {metrics['average_reward']:.4f}\")\n            logger.info(f\"  Overall F1 score: {metrics['overall_f1']:.4f}\")\n    \n    logger.info(\"Training complete. Performing final evaluation...\")\n    final_metrics = evaluate_performance(model, num_samples=200)\n    \n    # Generate learning curve\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(0, episodes, evaluation_interval), accuracies)\n    plt.xlabel('Episode')\n    plt.ylabel('Text Match Accuracy')\n    plt.title('Learning Curve - Accuracy')\n    \n    plt.subplot(1, 2, 2)\n    window_size = 100\n    smoothed_rewards = [sum(rewards[max(0, i-window_size):i])/min(i, window_size) \n                        for i in range(1, len(rewards)+1)]\n    plt.plot(smoothed_rewards)\n    plt.xlabel('Episode')\n    plt.ylabel('Average Reward (Moving Window)')\n    plt.title(f'Learning Curve - Reward (Window Size: {window_size})')\n    \n    plt.tight_layout()\n    plt.savefig('learning_curves.png')\n    \n    return model, final_metrics, best_model_path\n\nif __name__ == \"__main__\":\n    logger.info(\"Starting Sanskrit Librarian training program\")\n    \n    # Specify the arguments directly for Jupyter notebook\n    episodes = 5000\n    eval_interval = 500\n    \n    # Log system information\n    logger.info(f\"Training for {episodes} episodes with evaluation every {eval_interval} episodes\")\n    \n    # Start training\n    trained_model, metrics, best_model_path = train_sanskrit_librarian(\n        episodes=episodes, \n        evaluation_interval=eval_interval\n    )\n    \n    logger.info(\"Final performance metrics:\")\n    for key, value in metrics.items():\n        if not isinstance(value, dict) and not isinstance(value, list):\n            logger.info(f\"  {key}: {value}\")\n            \n    logger.info(\"Per-text F1 scores:\")\n    for text, text_metrics in metrics[\"per_text_metrics\"].items():\n        logger.info(f\"  {text}: {text_metrics['f1']:.4f}\")\n    \n    logger.info(f\"Best model saved at: {best_model_path}\")\n    logger.info(\"Training complete!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:55:07.214883Z","iopub.execute_input":"2025-05-24T15:55:07.215622Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/5000 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n\n  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n  2%|▏         | 1/50 [00:17<14:36, 17.89s/it]\u001b[A\n  4%|▍         | 2/50 [00:35<14:16, 17.85s/it]\u001b[A\n  6%|▌         | 3/50 [00:53<13:58, 17.83s/it]\u001b[A\n  8%|▊         | 4/50 [01:11<13:35, 17.73s/it]\u001b[A\n 10%|█         | 5/50 [01:28<13:20, 17.79s/it]\u001b[A\n 12%|█▏        | 6/50 [01:46<13:03, 17.80s/it]\u001b[A\n 14%|█▍        | 7/50 [02:04<12:42, 17.74s/it]\u001b[A\n 16%|█▌        | 8/50 [02:22<12:28, 17.81s/it]\u001b[A\n 18%|█▊        | 9/50 [02:40<12:08, 17.77s/it]\u001b[A\n 20%|██        | 10/50 [02:57<11:51, 17.79s/it]\u001b[A\n 22%|██▏       | 11/50 [03:15<11:33, 17.79s/it]\u001b[A\n 24%|██▍       | 12/50 [03:33<11:13, 17.73s/it]\u001b[A\n 26%|██▌       | 13/50 [03:51<10:56, 17.73s/it]\u001b[A\n 28%|██▊       | 14/50 [04:08<10:35, 17.65s/it]\u001b[A\n 30%|███       | 15/50 [04:26<10:18, 17.67s/it]\u001b[A\n 32%|███▏      | 16/50 [04:43<09:59, 17.64s/it]\u001b[A\n 34%|███▍      | 17/50 [05:01<09:42, 17.65s/it]\u001b[A\n 36%|███▌      | 18/50 [05:19<09:24, 17.65s/it]\u001b[A\n 38%|███▊      | 19/50 [05:36<09:05, 17.61s/it]\u001b[A\n 40%|████      | 20/50 [05:54<08:48, 17.60s/it]\u001b[A\n 42%|████▏     | 21/50 [06:11<08:28, 17.52s/it]\u001b[A\n 44%|████▍     | 22/50 [06:28<08:09, 17.48s/it]\u001b[A\n 46%|████▌     | 23/50 [06:46<07:52, 17.50s/it]\u001b[A\n 48%|████▊     | 24/50 [07:03<07:35, 17.50s/it]\u001b[A\n 50%|█████     | 25/50 [07:21<07:17, 17.52s/it]\u001b[A\n 52%|█████▏    | 26/50 [07:39<07:00, 17.52s/it]\u001b[A\n 54%|█████▍    | 27/50 [07:56<06:43, 17.55s/it]\u001b[A\n 56%|█████▌    | 28/50 [08:14<06:26, 17.55s/it]\u001b[A\n 58%|█████▊    | 29/50 [08:31<06:09, 17.58s/it]\u001b[A\n 60%|██████    | 30/50 [08:49<05:50, 17.55s/it]\u001b[A\n 62%|██████▏   | 31/50 [09:06<05:33, 17.57s/it]\u001b[A\n 64%|██████▍   | 32/50 [09:24<05:15, 17.55s/it]\u001b[A\n 66%|██████▌   | 33/50 [09:42<04:58, 17.55s/it]\u001b[A\n 68%|██████▊   | 34/50 [09:59<04:39, 17.49s/it]\u001b[A\n 70%|███████   | 35/50 [10:16<04:21, 17.41s/it]\u001b[A\n 72%|███████▏  | 36/50 [10:34<04:03, 17.43s/it]\u001b[A\n 74%|███████▍  | 37/50 [10:51<03:46, 17.46s/it]\u001b[A\n 76%|███████▌  | 38/50 [11:09<03:30, 17.51s/it]\u001b[A\n 78%|███████▊  | 39/50 [11:26<03:12, 17.46s/it]\u001b[A\n 80%|████████  | 40/50 [11:44<02:54, 17.48s/it]\u001b[A\n 82%|████████▏ | 41/50 [12:01<02:37, 17.48s/it]\u001b[A\n 84%|████████▍ | 42/50 [12:19<02:20, 17.50s/it]\u001b[A\n 86%|████████▌ | 43/50 [12:36<02:02, 17.53s/it]\u001b[A\n 88%|████████▊ | 44/50 [12:54<01:45, 17.52s/it]\u001b[A\n 90%|█████████ | 45/50 [13:11<01:27, 17.47s/it]\u001b[A\n 92%|█████████▏| 46/50 [13:29<01:10, 17.50s/it]\u001b[A\n 94%|█████████▍| 47/50 [13:46<00:52, 17.47s/it]\u001b[A\n 96%|█████████▌| 48/50 [14:04<00:34, 17.48s/it]\u001b[A\n 98%|█████████▊| 49/50 [14:21<00:17, 17.47s/it]\u001b[A\n100%|██████████| 50/50 [14:39<00:00, 17.58s/it]\u001b[A\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n  4%|▍         | 197/5000 [1:12:11<23:20:16, 17.49s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     logger.info(\"Starting Sanskrit Librarian training program\")\n    \n#     # Specify the arguments directly\n#     episodes = 5000\n#     eval_interval = 500\n    \n#     # Log system information\n#     logger.info(f\"Training for {episodes} episodes with evaluation every {eval_interval} episodes\")\n    \n#     # Start training\n#     trained_model, metrics, best_model_path = train_sanskrit_librarian(\n#         episodes=episodes, \n#         evaluation_interval=eval_interval\n#     )\n    \n#     logger.info(\"Final performance metrics:\")\n#     for key, value in metrics.items():\n#         if not isinstance(value, dict) and not isinstance(value, list):\n#             logger.info(f\"  {key}: {value}\")\n            \n#     logger.info(\"Per-text F1 scores:\")\n#     for text, text_metrics in metrics[\"per_text_metrics\"].items():\n#         logger.info(f\"  {text}: {text_metrics['f1']:.4f}\")\n    \n#     logger.info(f\"Best model saved at: {best_model_path}\")\n#     logger.info(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T15:21:06.188625Z","iopub.execute_input":"2025-05-24T15:21:06.189301Z","iopub.status.idle":"2025-05-24T15:21:06.212417Z","shell.execute_reply.started":"2025-05-24T15:21:06.189263Z","shell.execute_reply":"2025-05-24T15:21:06.211499Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2687970584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     trained_model, metrics, best_model_path = train_sanskrit_librarian(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mevaluation_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_sanskrit_librarian' is not defined"],"ename":"NameError","evalue":"name 'train_sanskrit_librarian' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}